{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian-based Saliency Map Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be implementing a JSMA, or Jacobian-based Saliency Map Attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the attack, let us first set up some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required packages\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from mnist_model_generator import Net\n",
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a model which is capable of classifying images from the MNIST dataset in advance.\n",
    "The details of this model can be found in the `mnist_model_generator.py` file, that is included in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our pre-trained MNIST model.\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('../../data/models/mnist_cnn.pt', map_location=torch.device('cpu')))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the image that we will be testing our attack on.\n",
    "three = Image.open(\"../../data/pictures/3.png\")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(28),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "three_tensor = preprocess(three)[0].reshape(1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image will be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../data/pictures/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run our model on the original image, to make sure the result is '3', as one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 3 with 100.0% certainty.\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {model(three_tensor).argmax().item()} with {model(three_tensor).max().item() * 100}% certainty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define what a JSMA is, following the approach by Papernote et al. as referenced in our report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSMA is a targeted attack, which means that we apply it on an image and tell the attack what output we want the model to have after the attack; we have a target.\n",
    "We will come back to this fact later on, in the analysis.\n",
    "\n",
    "We start by calculating the Jacobian of this image, with respect to the output tensor.\n",
    "\n",
    "This means the Jacobian will represent how much each individual pixel modifies the probability that each possible class is selected.\n",
    "\n",
    "Using the Jacobian, we will build a saliency map. Note that a saliency map can be used to either increase the pixel values that would improve the probability of our target class being selected, or to decrease the pixel values that would improve the probability of another class being selected.\n",
    "\n",
    "In the first case, the saliency map S is defined in the following way:\n",
    "\n",
    "![](images/S_positive.png)\n",
    "\n",
    "In the latter case, it is instead defined as follows:\n",
    "\n",
    "![](images/S_negative.png)\n",
    "\n",
    "The difference should be clear immediately, the first case looks for pixels which improve the chance of our target being selected while mostly decreasing the chances of other classes being selected. The latter does the exact opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(J, t, space, size, width):\n",
    "    S = [(0, 0)] * size\n",
    "    for p in space:\n",
    "        alpha = J[t, p // width, p % width].item()\n",
    "        beta = 0\n",
    "        for i in range(J.size(0)):\n",
    "            if not i == t:\n",
    "                beta += J[i, p // width, p % width].item()\n",
    "        S[p] = (alpha, beta)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a saliency map, based on either an increase of the value of 'good' pixels, or a decrease of the value of 'bad' pixels.\n",
    "\n",
    "We will then search for a pair of pixels, such that their total maximally increases the chances of our target being selected, while minimizing the chance other classes get selected, or the inverse if we are working with the decreasing variant.\n",
    "\n",
    "After finding this pair, we will set this pixel value to 1 if increasing and 0 if decreasing.\n",
    "\n",
    "We will remove this pixel from our search space, such that we never touch it again later in the algorithm.\n",
    "\n",
    "We will repeat this process until one of three cases occurs:\n",
    "- The model predicts our target class instead of the actual class.\n",
    "- Our image has been modified more than some threshold `max_dist` allows. This means the attack has failed to create an adversarial example for this input. This is interpreted as a maximum number of iterations, the simple calculation for this is explained in Papernot et al.\n",
    "- We have modified each pixel. This is a clear failure to create an adversarial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma(original_image_tensor, target, predictor, max_dist, increase, normalize=True):\n",
    "    img_tensor = original_image_tensor.clone()\n",
    "\n",
    "    # Normalize the data to the range [0,1] before the attack\n",
    "    if normalize:\n",
    "        img_tensor = img_tensor.reshape(28,28)\n",
    "        \n",
    "        min_val = torch.min(img_tensor.reshape(784))\n",
    "        max_val = torch.max(img_tensor.reshape(784))\n",
    "\n",
    "        img_tensor = torch.sub(img_tensor, min_val)\n",
    "        img_tensor = torch.div(img_tensor, max_val - min_val)\n",
    "\n",
    "    img_tensor = img_tensor.reshape(1,1,28,28)\n",
    "    \n",
    "    img_size = img_tensor.size(2) * img_tensor.size(3)\n",
    "    width = img_tensor.size(3)\n",
    "    search_space = list(range(img_size))\n",
    "    i = 0\n",
    "    max_iter = math.floor((img_size * max_dist) / (200))\n",
    "    chosen_pixel_1 = -1\n",
    "    chosen_pixel_2 = -1\n",
    "    prediction = predictor(img_tensor)\n",
    "\n",
    "    while not prediction.argmax().item() == target and i < max_iter and len(search_space) >= 2:\n",
    "        max = 0\n",
    "        # Generate the Jacobian\n",
    "        J = torch.autograd.functional.jacobian(predictor, img_tensor)[0, :, 0, 0, :, :]\n",
    "\n",
    "        #Generate the Saliency map\n",
    "        S = saliency_map(J, target, search_space, img_size, width)\n",
    "\n",
    "        # Find the optimal pair of pixels\n",
    "        for pixel1 in search_space:\n",
    "            for pixel2 in search_space:\n",
    "                if pixel1 == pixel2:\n",
    "                    continue\n",
    "                \n",
    "                alpha = S[pixel1][0] + S[pixel2][0]\n",
    "                beta = S[pixel1][1] + S[pixel2][1]\n",
    "\n",
    "                sign_check = alpha > 0 and beta < 0 if increase else alpha < 0 and beta > 0\n",
    "                if sign_check and -alpha * beta > max:\n",
    "                    chosen_pixel_1 = pixel1\n",
    "                    chosen_pixel_2 = pixel2\n",
    "                    max = -alpha * beta\n",
    "\n",
    "        # No pair found that would improve the current state.\n",
    "        if max == 0:\n",
    "            break\n",
    "\n",
    "        # Adjust the pixel values according to which version we use.\n",
    "        img_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width] = 1 if increase else 0\n",
    "        img_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width] = 1 if increase else 0\n",
    "\n",
    "        # Remove the chosen pixels from the search space.\n",
    "        search_space.remove(chosen_pixel_1)\n",
    "        search_space.remove(chosen_pixel_2)\n",
    "        \n",
    "        # Predict the current adversarial image to check whether we need to continue.\n",
    "        prediction = predictor(img_tensor)\n",
    "        i += 1\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have worked through the JSMA implementation, we can now test it.\n",
    "\n",
    "We will run this attack on our example image once, with every target, besides the actual class.\n",
    "\n",
    "Additionally, we will run both the increase and decrease variant.\n",
    "\n",
    "Finally, the following experiment was done by Papernot et al. and we thought it was interesting enough to recreate it and briefly analyze its results.\n",
    "This experiments consists of giving our attack a target, but supplying an empty image.\n",
    "This will ask the attack to generate the minimal requirement in an image to predict a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified as 0 with goal 0\n",
      "Classified as 1 with goal 1\n",
      "Classified as 2 with goal 2\n",
      "Classified as 4 with goal 4\n",
      "Classified as 5 with goal 5\n",
      "Classified as 6 with goal 6\n",
      "Classified as 7 with goal 7\n",
      "Classified as 8 with goal 8\n",
      "Classified as 9 with goal 9\n",
      "Classified as 0 with goal 0\n",
      "Classified as 1 with goal 1\n",
      "Classified as 2 with goal 2\n",
      "Classified as 4 with goal 4\n",
      "Classified as 5 with goal 5\n",
      "Classified as 6 with goal 6\n",
      "Classified as 7 with goal 7\n",
      "Classified as 8 with goal 8\n",
      "Classified as 9 with goal 9\n",
      "Classified as 0 with goal 0\n",
      "Classified as 1 with goal 1\n",
      "Classified as 2 with goal 2\n",
      "Classified as 3 with goal 3\n",
      "Classified as 4 with goal 4\n",
      "Classified as 5 with goal 5\n",
      "Classified as 6 with goal 6\n",
      "Classified as 7 with goal 7\n",
      "Classified as 8 with goal 8\n",
      "Classified as 9 with goal 9\n"
     ]
    }
   ],
   "source": [
    "attacked_models_positive = []\n",
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        attacked_models_positive.append(None)\n",
    "        continue\n",
    "    attacked_models_positive.append(jsma(three_tensor, i, model, 20, True))\n",
    "    print(f'Classified as {model(attacked_models_positive[i]).argmax().item()} with goal {i}')\n",
    "    save_image(attacked_models_positive[i][0,0], f'../../results/JSMA/positive-{i}.png')\n",
    "\n",
    "attacked_models_negative = []\n",
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        attacked_models_negative.append(None)\n",
    "        continue\n",
    "    attacked_models_negative.append(jsma(three_tensor, i, model, 20, False))\n",
    "    print(f'Classified as {model(attacked_models_negative[i]).argmax().item()} with goal {i}')\n",
    "    save_image(attacked_models_negative[i][0,0], f'../../results/JSMA/negative-{i}.png')\n",
    "\n",
    "attacked_models_empty = []\n",
    "for i in range(10):\n",
    "    attacked_models_empty.append(jsma(torch.zeros_like(three_tensor), i, model, 20, True, False))\n",
    "    print(f'Classified as {model(attacked_models_empty[i]).argmax().item()} with goal {i}')\n",
    "    save_image(attacked_models_empty[i][0,0], f'../../results/JSMA/empty-{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many results were generated by this experiment, we will only analyze the three most interesting results of both the increase and decrease variants.\n",
    "Additionally, we will select only two results from the final experiment, as all results are similar, yet one sticks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the positive variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the output of the previous experiments, the positive variant was able to fool the model with a 100% success rate.\n",
    "We will now display three of the most interesting results:\n",
    "\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/positive-2.png width=140>\n",
    "    <figcaption>Classified as a 2</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/positive-8.png width=140>\n",
    "    <figcaption>Classified as a 8</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/positive-4.png width=140>\n",
    "    <figcaption>Classified as a 4</figcaption>\n",
    "</figure>\n",
    "\n",
    "As you can see, while the classification has been fooled, such that it has given the incorrect prediction, the images are clearly modified.\n",
    "Humans may also fall for some of these illusions.\n",
    "\n",
    "The first one is classified as a 2, while being an image of a 3.\n",
    "The issue here is, humans could argue that this image no longer shows a clear 3, as it resembles a mirrored 6.\n",
    "For an optimal attack, it should be difficult for humans to notice that an attack has occured at all.\n",
    "\n",
    "Similarly, the second image is classified as an 8, while being an image of a 3.\n",
    "We can clearly see why the model considers it an 8, as the pixels that have been added have made the 3 look like an 8.\n",
    "Therefore, once again, while the prediction is incorrect, we cannot state with confidence that we have fooled the model.\n",
    "\n",
    "Finally, the last image is classified as a 4, while being an image of a 3.\n",
    "This image has been modified to such an extent, that a reasonable percentage of human users, would consider this adversarial example a 9, rather than a 3.\n",
    "So, once again, we cannot fault the model for being tricked by this image.\n",
    "\n",
    "From these results, one might think to decrease the `max_dist` parameter, as to make the resulting images remain true to the original.\n",
    "However, with lower `max_dist` values, the resulting images did not get much less messy, and the success rate dropped by a noticeable amount.\n",
    "Therefore, while the positive JSMA is effective at creating an adversarial example that will be classified incorrectly, it modifies the original image too much to be considered as a powerful attack.\n",
    "\n",
    "One possible improvement would be to decrease the amount with which we increase the optimal pair of pixels.\n",
    "However, Papernot et al. stated that a maximum increase would yield optimal results, so we stuck with this assumption.\n",
    "Instead, lower jumps were explored in another attack we performed, namely, the improvement on JSMA: the JSMA-M attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the negative variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the output of the previous experiments, the negative variant was able to fool the model with a 100% success rate as well.\n",
    "We will now display three of the most interesting results:\n",
    "\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/negative-2.png width=140>\n",
    "    <figcaption>Classified as a 2</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/negative-5.png width=140>\n",
    "    <figcaption>Classified as a 5</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/negative-1.png width=140>\n",
    "    <figcaption>Classified as a 1</figcaption>\n",
    "</figure>\n",
    "\n",
    "We immediately notice that none of these images represent another number more than they represent 3, which is an improvement from the previous experiment.\n",
    "\n",
    "However, another issue becomes clear.\n",
    "Some of the images have been attacked to the point of not clearly representing a 3 anymore.\n",
    "Take the final image as an example, which is classified as a 1.\n",
    "We have fooled the model into thinking this is a 1, however, we have ruined the image in the process.\n",
    "\n",
    "However, unlike the previous experiments, not all images have the same downside.\n",
    "The images that were classified as 2 and 5 are both not attacked to the point where they are unrecognizable.\n",
    "It is clear that an attack has occurred, however, any human would still classify these images as a 3, while the model claims otherwise.\n",
    "\n",
    "This can be considered a successful attack.\n",
    "\n",
    "Ofcourse, whether the positive or negative variant is more successful also depends on the input.\n",
    "For a 3, it makes sense that removing pixel values would be more effective than adding pixel values.\n",
    "\n",
    "On the other hand, for a 1, for example, adding pixel values to the image would have a much larger chance to succeed.\n",
    "\n",
    "This is the downside of targeted attacks.\n",
    "While they can be very powerful, human coaching is needed to get proper results.\n",
    "But when this human guidance is given, we can tell from the experiments that this method can indeed fool a model by generating an adversarial example.\n",
    "In the notebook on JSMA-M, we will instead analyze a non-targeted approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Analyzing the empty variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as mentioned during the attacking stage, we applied the positive attack on an empty image in order to view the most distinctive features of a given class.\n",
    "Almost all classes behaved fairly similarly, showing us exactly which pixel values are representative of a given class.\n",
    "An example of this is the following, which was an attempt at classifying the empty image as a 4.\n",
    "\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/empty-4.png width=140>\n",
    "    <figcaption>Classified as a 4</figcaption>\n",
    "</figure>\n",
    "\n",
    "It becomes clear that these two pixels are essential for a proper 4.\n",
    "This makes sense, as no other numbers have the sharp edges on the sides in the middle of the image.\n",
    "\n",
    "However, one image returned a surprising result.\n",
    "Namely, the attempt to create an adversarial example that makes the model see a 1.\n",
    "\n",
    "<figure>\n",
    "    <img src=../../results/JSMA/empty-1.png width=140>\n",
    "    <figcaption>Classified as a 1</figcaption>\n",
    "</figure>\n",
    "\n",
    "As you can see, the image is the exact same as the input image we gave.\n",
    "This means that, for an empty image, the model, defaults to 1.\n",
    "This could be explained by the fact that 1 is the smallest number, and therefore, when not enough information is present, the model reverts to the class that requires the least information: the 1.\n",
    "This was an interesting result that was more-or-less also achieved by Papernot et al., which is why we thought it would be interesting to share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effectiveness of adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have seen that this attack is capable of tricking our model into predicting the wrong class with a very high success rate.\n",
    "\n",
    "Adversarial training is one of the methods that can be used to defend a model against incoming attacks.\n",
    "\n",
    "However, we run into a small issue.\n",
    "This attack is a targeted attack, and we have seen how, with improper guidance, this attack will not be effective at all, and instead ruin the input image.\n",
    "Therefore it would be difficult to properly train a model to resist our attacks without spending weeks manually setting up an attack for the training data.\n",
    "\n",
    "Therefore, we choose to borrow the robust model that was generated in the FGSM & similar attacks notebook.\n",
    "While this will ofcourse not be trained specifically against our attack, we will explore whether it performs petter than our original model either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the robust MNIST model.\n",
    "def_model = Net()\n",
    "def_model.load_state_dict(torch.load('../../data/models/mnist_robust.pt', map_location=torch.device('cpu')))\n",
    "def_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us make sure this model still properly recognises an unmodified input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 3 with 100.0% certainty.\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {def_model(three_tensor).argmax().item()} with {model(three_tensor).max().item() * 100}% certainty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model still works as it should.\n",
    "\n",
    "Now, let us attempt to run the same experiments as before and compare their success rate.\n",
    "\n",
    "We will not repeat the empty inputs, as this will not tell us anything about the robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified as 0 with goal 0\n",
      "Classified as 1 with goal 1\n",
      "Classified as 2 with goal 2\n",
      "Classified as 4 with goal 4\n",
      "Classified as 5 with goal 5\n",
      "Classified as 8 with goal 6\n",
      "Classified as 7 with goal 7\n",
      "Classified as 8 with goal 8\n",
      "Classified as 9 with goal 9\n",
      "Classified as 0 with goal 0\n",
      "Classified as 1 with goal 1\n",
      "Classified as 2 with goal 2\n",
      "Classified as 3 with goal 4\n",
      "Classified as 5 with goal 5\n",
      "Classified as 6 with goal 6\n",
      "Classified as 7 with goal 7\n",
      "Classified as 8 with goal 8\n",
      "Classified as 9 with goal 9\n"
     ]
    }
   ],
   "source": [
    "defended_models_positive = []\n",
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        defended_models_positive.append(None)\n",
    "        continue\n",
    "    defended_models_positive.append(jsma(three_tensor, i, def_model, 20, True))\n",
    "    print(f'Classified as {def_model(defended_models_positive[i]).argmax().item()} with goal {i}')\n",
    "    save_image(defended_models_positive[i][0,0], f'../../results/JSMA/defended-positive-{i}.png')\n",
    "\n",
    "defended_models_negative = []\n",
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        defended_models_negative.append(None)\n",
    "        continue\n",
    "    defended_models_negative.append(jsma(three_tensor, i, def_model, 20, False))\n",
    "    print(f'Classified as {def_model(defended_models_negative[i]).argmax().item()} with goal {i}')\n",
    "    save_image(defended_models_negative[i][0,0], f'../../results/JSMA/defended-negative-{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results are stored, and can be viewed inside the results folder in the main repository.\n",
    "\n",
    "Here, we will make a general statement about the effectiveness of the adversarially trained model, as the same can be said about nearly all results.\n",
    "\n",
    "The first thing we must notice, is that the success rate of our attack is no longer 100%.\n",
    "Instead the attack failed twice!\n",
    "\n",
    "Additionally, we notice that the execution time of the attack against the robust model is nearly twice as long.\n",
    "While this is not important when it comes to the security of our model, it does show that the attack struggled to fool this model.\n",
    "\n",
    "This can also be seen in the results, which, though having the same results as our original model, look far more messy now.\n",
    "\n",
    "The fact that the negative JSMA failed to convince the robust model to predict the 3 as a 4, and instead predicted it as a 3, shows that adversarial training has the potential to partially protect our model from adversarial inputs.\n",
    "\n",
    "Please see the report for a discussion on how this attack compares to the other tested attacks."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d185d99660d43f918a309554ca434be2526a3d95ca2e0b28b0f0a334f8c90ee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
