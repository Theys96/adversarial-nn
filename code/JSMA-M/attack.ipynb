{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximal Jacobian-based Saliency Map Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be implementing a M-JSMA, or Maximal Jacobian-based Saliency Map Attack.\n",
    "\n",
    "Before reading through this notebook, please read through the regular JSMA notebook in `code/JSMA/attack.ipynb` first, as some concepts were discussed here that will be considered to be known to the user in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will start by setting up our standard model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from mnist_model_generator import Net\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our pre-trained MNIST model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('../../data/models/mnist_cnn.pt', map_location=torch.device('cpu')))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the 3 images we will be testing on.\n",
    "# We will use different images, as this attack method is untargeted.\n",
    "# Notice this attack is much more effective without prior normalization.\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(28),\n",
    "   transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "three = Image.open(\"../../data/pictures/3.png\")\n",
    "three_tensor = preprocess(three)[0].reshape(1,1,28,28)\n",
    "\n",
    "four = Image.open(\"../../data/pictures/4.png\")\n",
    "four_tensor = preprocess(four)[0].reshape(1,1,28,28)\n",
    "\n",
    "eight = Image.open(\"../../data/pictures/8.png\")\n",
    "eight_tensor = preprocess(eight)[0].reshape(1,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following images will be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../../data/pictures/3.png width=140>\n",
    "<img src=../../data/pictures/4.png width=140>\n",
    "<img src=../../data/pictures/8.png width=140>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us test our model to make sure it correctly predicts these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 3 with 99.99912977218628% certainty. Should be 3\n",
      "The model predicted: 4 with 91.91465973854065% certainty. Should be 4\n",
      "The model predicted: 8 with 99.97344613075256% certainty. Should be 8\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {model(three_tensor).argmax().item()} with {model(three_tensor).max().item() * 100}% certainty. Should be 3')\n",
    "print(f'The model predicted: {model(four_tensor).argmax().item()} with {model(four_tensor).max().item() * 100}% certainty. Should be 4')\n",
    "print(f'The model predicted: {model(eight_tensor).argmax().item()} with {model(eight_tensor).max().item() * 100}% certainty. Should be 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by discussing the changes that M-JSMA made to the original JSMA.\n",
    "- First of all, the attack is now non-targeted, as it checks that the predicted class is not the actual class, rather than checking whether it is the target class.\n",
    "- This attack combines the positive and negative saliency maps, using the positive whenever pixels that improve classes that are not the actual class are chosen and using the negative whenever pixels that improve the actual class are chosen.\n",
    "- The modification value is chosen to be a lower value and therefore, pixels can be adjusted more than once. This leads to adversarial images that appear 'less attacked'\n",
    "\n",
    "Though appearing as not many changes, this has tremendous effects on the attack as a whole.\n",
    "\n",
    "The fact that the attack is able to create images that appear to be attacked less, as well as doing so without guidance from a human, means it becomes applicable in many more areas that previously possible.\n",
    "\n",
    "The code for M-JSMA can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(J, t, space, size, width):\n",
    "    S = [(0, 0)] * size\n",
    "    for p in space:\n",
    "        alpha = J[t, p // width, p % width].item()\n",
    "        beta = 0\n",
    "        for i in range(J.size(0)):\n",
    "            if not i == t:\n",
    "                beta += J[i, p // width, p % width].item()\n",
    "        S[p] = (alpha, beta)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(orig, val, eps):\n",
    "    return min([1, orig + eps, max([0, orig - eps, val])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsmaM(original_image_tensor, actual_class, predictor, max_dist, perturbation, epsilon):\n",
    "    img_tensor = original_image_tensor.clone()\n",
    "\n",
    "    img_tensor = img_tensor.reshape(1,1,28,28)\n",
    "\n",
    "    img_size = img_tensor.size(2) * img_tensor.size(3)\n",
    "    width = img_tensor.size(3)\n",
    "    search_space = list(range(img_size))\n",
    "    i = 0\n",
    "    max_iter = math.floor((img_size * max_dist) / (200))\n",
    "    chosen_pixel_1 = -1\n",
    "    chosen_pixel_2 = -1\n",
    "    modifier = 0\n",
    "    prediction = predictor(img_tensor)\n",
    "\n",
    "    # Eta denotes the most recent change to each pixel, to make sure no cycles are formed.\n",
    "    eta = [0] * img_size\n",
    "\n",
    "    while prediction.argmax().item() == actual_class and i < max_iter and len(search_space) >= 2:\n",
    "        max = 0\n",
    "\n",
    "        # Calculate the jacobian.\n",
    "        J = torch.autograd.functional.jacobian(predictor, img_tensor)[0, :, 0, 0, :, :]\n",
    "\n",
    "        # Calculate the saliency map.\n",
    "        S = [saliency_map(J, target, search_space, img_size, width) for target in range(10)]\n",
    "        \n",
    "        # For all possible classes\n",
    "        for t in range(10):\n",
    "            # Find the optimal pixel pair for class t.\n",
    "            for pixel1 in search_space:\n",
    "                for pixel2 in search_space:\n",
    "                    if pixel1 == pixel2:\n",
    "                        continue\n",
    "                    \n",
    "                    alpha = S[t][pixel1][0] + S[t][pixel2][0]\n",
    "                    beta = S[t][pixel1][1] + S[t][pixel2][1]\n",
    "\n",
    "                    if -alpha * beta > max:\n",
    "                        chosen_pixel_1 = pixel1\n",
    "                        chosen_pixel_2 = pixel2\n",
    "                        max = -alpha * beta\n",
    "                        # If the most influential pixels boost the chances of the actual class, we wish to use the negative JSMA\n",
    "                        # Otherwise, we use the positive JSMA.\n",
    "                        modifier = (-1 if t == actual_class else 1) * math.copysign(1, alpha) * perturbation\n",
    "\n",
    "        # If no improvements can be made, quit.\n",
    "        if max == 0:\n",
    "            break\n",
    "        \n",
    "        # Make sure the change remains within some epsilon of the original value.\n",
    "        # This helps to keep the image as close to the original as possible.\n",
    "        new1 = clip(\n",
    "            original_image_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width].item(),\n",
    "            img_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width].item() + modifier,\n",
    "            epsilon\n",
    "        )\n",
    "        diff1 = abs(new1 - img_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width])\n",
    "        img_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width] = new1\n",
    "        \n",
    "        new2 = clip(\n",
    "            original_image_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width].item(),\n",
    "            img_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width].item() + modifier,\n",
    "            epsilon\n",
    "        )\n",
    "        diff2 = abs(new2 - img_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width])\n",
    "        img_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width] = new2\n",
    "\n",
    "        # Remove the pixels from the search domain under some conditions:\n",
    "        #   - The new value is 0 or 1\n",
    "        #   - The new value is the same as the old value\n",
    "        #   - A loop was found using eta.\n",
    "        val = img_tensor[0, 0, chosen_pixel_1 // width, chosen_pixel_1 % width]\n",
    "        if val <= 0 or val >= 1 or diff1 < 1e-06 or eta[chosen_pixel_1] == -1 * modifier:\n",
    "            search_space.remove(chosen_pixel_1)\n",
    "    \n",
    "        val = img_tensor[0, 0, chosen_pixel_2 // width, chosen_pixel_2 % width]\n",
    "        if val == 0 or val == 1 or diff2 < 1e-06 or eta[chosen_pixel_2] == -1 * modifier:\n",
    "            search_space.remove(chosen_pixel_2)\n",
    "        \n",
    "        eta[chosen_pixel_1] = modifier\n",
    "        eta[chosen_pixel_2] = modifier\n",
    "        prediction = predictor(img_tensor)\n",
    "\n",
    "        # Print some statements to show the attack's progress.\n",
    "        topPredictions = torch.topk(prediction, 2).indices[0]\n",
    "        closestIndex = topPredictions[1].item() if prediction.argmax() == actual_class else topPredictions[0].item()\n",
    "        print(f'Actual class: {actual_class} at {prediction[0, actual_class] * 100}%')\n",
    "        print(f'Closest attack: {closestIndex} at {prediction[0, closestIndex] * 100}%')\n",
    "\n",
    "        i += 1\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 8 at 99.92290496826172%\n",
      "Closest attack: 2 at 0.03664131462574005%\n",
      "Actual class: 8 at 99.86494445800781%\n",
      "Closest attack: 2 at 0.08285068720579147%\n",
      "Actual class: 8 at 99.81197357177734%\n",
      "Closest attack: 2 at 0.1156361997127533%\n",
      "Actual class: 8 at 99.40975952148438%\n",
      "Closest attack: 2 at 0.5095492601394653%\n",
      "Actual class: 8 at 99.40975952148438%\n",
      "Closest attack: 2 at 0.5095492601394653%\n",
      "Actual class: 8 at 99.1590576171875%\n",
      "Closest attack: 2 at 0.7626803517341614%\n",
      "Actual class: 8 at 99.04500579833984%\n",
      "Closest attack: 2 at 0.8696763515472412%\n",
      "Actual class: 8 at 97.74250030517578%\n",
      "Closest attack: 2 at 2.0979902744293213%\n",
      "Actual class: 8 at 96.52853393554688%\n",
      "Closest attack: 2 at 3.3512566089630127%\n",
      "Actual class: 8 at 95.88655853271484%\n",
      "Closest attack: 2 at 3.9759793281555176%\n",
      "Actual class: 8 at 94.82962799072266%\n",
      "Closest attack: 2 at 5.0015034675598145%\n",
      "Actual class: 8 at 92.57537078857422%\n",
      "Closest attack: 2 at 7.203100681304932%\n",
      "Actual class: 8 at 88.50079345703125%\n",
      "Closest attack: 2 at 11.217619895935059%\n",
      "Actual class: 8 at 84.6458969116211%\n",
      "Closest attack: 2 at 15.033008575439453%\n",
      "Actual class: 8 at 70.985107421875%\n",
      "Closest attack: 2 at 28.603103637695312%\n",
      "Actual class: 8 at 63.87141418457031%\n",
      "Closest attack: 2 at 35.66487121582031%\n",
      "Actual class: 8 at 49.721675872802734%\n",
      "Closest attack: 2 at 49.734100341796875%\n",
      "\n",
      "Actual class: 4 at 91.91465759277344%\n",
      "Closest attack: 9 at 7.618483066558838%\n",
      "Actual class: 4 at 91.62653350830078%\n",
      "Closest attack: 9 at 7.894159317016602%\n",
      "Actual class: 4 at 84.89642333984375%\n",
      "Closest attack: 9 at 14.602239608764648%\n",
      "Actual class: 4 at 84.89642333984375%\n",
      "Closest attack: 9 at 14.602239608764648%\n",
      "Actual class: 4 at 80.12345123291016%\n",
      "Closest attack: 9 at 19.158645629882812%\n",
      "Actual class: 4 at 66.99078369140625%\n",
      "Closest attack: 9 at 32.0536003112793%\n",
      "Actual class: 4 at 44.372398376464844%\n",
      "Closest attack: 9 at 54.697723388671875%\n",
      "\n",
      "Actual class: 3 at 99.99703979492188%\n",
      "Closest attack: 5 at 0.0021839598193764687%\n",
      "Actual class: 3 at 99.98995971679688%\n",
      "Closest attack: 5 at 0.006968570873141289%\n",
      "Actual class: 3 at 99.98995971679688%\n",
      "Closest attack: 5 at 0.006968570873141289%\n",
      "Actual class: 3 at 99.95873260498047%\n",
      "Closest attack: 5 at 0.02602723240852356%\n",
      "Actual class: 3 at 99.88292694091797%\n",
      "Closest attack: 5 at 0.06760978698730469%\n",
      "Actual class: 3 at 99.88292694091797%\n",
      "Closest attack: 5 at 0.06760978698730469%\n",
      "Actual class: 3 at 99.8077392578125%\n",
      "Closest attack: 5 at 0.11071012169122696%\n",
      "Actual class: 3 at 99.7137451171875%\n",
      "Closest attack: 5 at 0.15590371191501617%\n",
      "Actual class: 3 at 99.28763580322266%\n",
      "Closest attack: 9 at 0.353429913520813%\n",
      "Actual class: 3 at 99.14041900634766%\n",
      "Closest attack: 9 at 0.42661967873573303%\n",
      "Actual class: 3 at 98.77963256835938%\n",
      "Closest attack: 9 at 0.6788085103034973%\n",
      "Actual class: 3 at 97.75202941894531%\n",
      "Closest attack: 9 at 1.3903305530548096%\n",
      "Actual class: 3 at 96.98590850830078%\n",
      "Closest attack: 9 at 2.013829231262207%\n",
      "Actual class: 3 at 94.3895263671875%\n",
      "Closest attack: 9 at 4.105161666870117%\n",
      "Actual class: 3 at 89.97945404052734%\n",
      "Closest attack: 9 at 8.049114227294922%\n",
      "Actual class: 3 at 86.40121459960938%\n",
      "Closest attack: 9 at 11.336256980895996%\n",
      "Actual class: 3 at 82.39195251464844%\n",
      "Closest attack: 9 at 14.938685417175293%\n",
      "Actual class: 3 at 78.0799331665039%\n",
      "Closest attack: 9 at 19.097871780395508%\n",
      "Actual class: 3 at 64.09912872314453%\n",
      "Closest attack: 9 at 32.727577209472656%\n",
      "Actual class: 3 at 64.09912872314453%\n",
      "Closest attack: 9 at 32.727577209472656%\n",
      "Actual class: 3 at 57.28783416748047%\n",
      "Closest attack: 9 at 39.26219177246094%\n",
      "Actual class: 3 at 52.62441635131836%\n",
      "Closest attack: 9 at 43.56866455078125%\n",
      "Actual class: 3 at 51.98997116088867%\n",
      "Closest attack: 9 at 44.18288040161133%\n",
      "Actual class: 3 at 51.98997116088867%\n",
      "Closest attack: 9 at 44.18288040161133%\n",
      "Actual class: 3 at 49.80400085449219%\n",
      "Closest attack: 9 at 46.601802825927734%\n",
      "Actual class: 3 at 41.89406967163086%\n",
      "Closest attack: 9 at 53.72508239746094%\n"
     ]
    }
   ],
   "source": [
    "attacked_3 = jsmaM(three_tensor, 3, model, 20, 1, 0.5)\n",
    "save_image(attacked_3[0,0], '../../results/JSMA-M/attacked-3.png')\n",
    "print('')\n",
    "attacked_4 = jsmaM(four_tensor, 4, model, 20, 1, 0.5)\n",
    "save_image(attacked_4[0,0], '../../results/JSMA-M/attacked-4.png')\n",
    "print('')\n",
    "attacked_8 = jsmaM(eight_tensor, 8, model, 20, 1, 0.5)\n",
    "save_image(attacked_8[0,0], '../../results/JSMA-M/attacked-8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 9 with 53.7250816822052% certainty. Should be 3\n",
      "The model predicted: 9 with 54.69772219657898% certainty. Should be 4\n",
      "The model predicted: 2 with 49.734100699424744% certainty. Should be 8\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {model(attacked_3).argmax().item()} with {model(attacked_3).max().item() * 100}% certainty. Should be 3')\n",
    "print(f'The model predicted: {model(attacked_4).argmax().item()} with {model(attacked_4).max().item() * 100}% certainty. Should be 4')\n",
    "print(f'The model predicted: {model(attacked_8).argmax().item()} with {model(attacked_8).max().item() * 100}% certainty. Should be 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following three results, as can be seen in the print statements above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=../../results/JSMA-M/attacked-3.png width=140>\n",
    "    <figcaption>Classified as a 9</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA-M/attacked-4.png width=140>\n",
    "    <figcaption>Classified as a 9</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA-M/attacked-8.png width=140>\n",
    "    <figcaption>Classified as a 2</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first image, we see the same issue we had with JSMA's generated adversarial example.\n",
    "A human could very well mistake this image for a 9 as well, though less so due to the area that 'closes' the 9 being less bright than the rest of the 3.\n",
    "\n",
    "However, we notice that the other two images would without a doubt still be classified as the correct number by human observers.\n",
    "Yet the model failed to properly classify them.\n",
    "This means our attack was successful!\n",
    "\n",
    "These results are certainly an improvement over those generated by JSMA.\n",
    "Additionally, this method in a non-targeted attack, which means less human interaction is requires, which is a great bonus for an adversarial example generator.\n",
    "\n",
    "Let us finalize by giving M-JSMA a chance against the robust model, as mentioned before in the JSMA notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effectiveness of adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that this attack is capable of tricking our model into predicting the wrong class with a very high success rate.\n",
    "\n",
    "Adversarial training is one of the methods that can be used to defend a model against incoming attacks.\n",
    "\n",
    "This time, our attack is indeed non-targeted, so it could be used to train a model.\n",
    "However, another issue comes up.\n",
    "\n",
    "Namely, the execution time of this attack is not as low as it was for simpler attacks.\n",
    "The training set consists of 60000 images, with an average time per image of about 80 seconds, the training would take 55.5 days.\n",
    "We simply do not have the time to create this model, even though it would likely be a very robust model.\n",
    "\n",
    "Therefore, we once again choose to borrow the robust model that was generated in the FGSM & similar attacks notebook.\n",
    "While this will ofcourse not be trained specifically against our attack, we will explore whether it performs better than our original model either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the robust MNIST model.\n",
    "def_model = Net()\n",
    "def_model.load_state_dict(torch.load('../../data/models/mnist_robust.pt', map_location=torch.device('cpu')))\n",
    "def_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let us make sure this model functions properly for regular images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 3 with 100.0% certainty. Should be 3\n",
      "The model predicted: 4 with 98.1884241104126% certainty. Should be 4\n",
      "The model predicted: 8 with 100.0% certainty. Should be 8\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {def_model(three_tensor).argmax().item()} with {def_model(three_tensor).max().item() * 100}% certainty. Should be 3')\n",
    "print(f'The model predicted: {def_model(four_tensor).argmax().item()} with {def_model(four_tensor).max().item() * 100}% certainty. Should be 4')\n",
    "print(f'The model predicted: {def_model(eight_tensor).argmax().item()} with {def_model(eight_tensor).max().item() * 100}% certainty. Should be 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news, the robust model performs even better than our original model!\n",
    "\n",
    "Next, let us run the exact same attack as before on this model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 1.16762207541532e-11%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 4.471123471461169e-11%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 2.8891170011924316e-10%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 1.3701034751179009e-09%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 1.4401884129711107e-07%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 1.4401884129711107e-07%\n",
      "Actual class: 3 at 100.0%\n",
      "Closest attack: 2 at 1.4657595102107734e-06%\n",
      "Actual class: 3 at 99.99998474121094%\n",
      "Closest attack: 2 at 6.159306394692976e-06%\n",
      "Actual class: 3 at 99.99996185302734%\n",
      "Closest attack: 2 at 3.345565710333176e-05%\n",
      "Actual class: 3 at 99.99990844726562%\n",
      "Closest attack: 2 at 9.704021067591384e-05%\n",
      "Actual class: 3 at 99.99979400634766%\n",
      "Closest attack: 2 at 0.00019738267292268574%\n",
      "Actual class: 3 at 99.99881744384766%\n",
      "Closest attack: 2 at 0.0011575249955058098%\n",
      "Actual class: 3 at 99.99321746826172%\n",
      "Closest attack: 2 at 0.006664319429546595%\n",
      "Actual class: 3 at 99.96385192871094%\n",
      "Closest attack: 2 at 0.035532109439373016%\n",
      "Actual class: 3 at 99.86322784423828%\n",
      "Closest attack: 2 at 0.1358392834663391%\n",
      "Actual class: 3 at 99.3958969116211%\n",
      "Closest attack: 2 at 0.6022660732269287%\n",
      "Actual class: 3 at 97.66754150390625%\n",
      "Closest attack: 2 at 2.327864408493042%\n",
      "Actual class: 3 at 97.66754150390625%\n",
      "Closest attack: 2 at 2.327864408493042%\n",
      "Actual class: 3 at 95.18497467041016%\n",
      "Closest attack: 2 at 4.80820894241333%\n",
      "Actual class: 3 at 95.18497467041016%\n",
      "Closest attack: 2 at 4.80820894241333%\n",
      "Actual class: 3 at 95.18497467041016%\n",
      "Closest attack: 2 at 4.80820894241333%\n",
      "Actual class: 3 at 91.30697631835938%\n",
      "Closest attack: 2 at 8.686956405639648%\n",
      "Actual class: 3 at 83.39636993408203%\n",
      "Closest attack: 2 at 16.596351623535156%\n",
      "Actual class: 3 at 72.6840591430664%\n",
      "Closest attack: 2 at 27.310588836669922%\n",
      "Actual class: 3 at 58.415000915527344%\n",
      "Closest attack: 2 at 41.579010009765625%\n",
      "Actual class: 3 at 31.21953582763672%\n",
      "Closest attack: 2 at 68.77274322509766%\n",
      "\n",
      "Actual class: 4 at 97.92335510253906%\n",
      "Closest attack: 9 at 2.0765416622161865%\n",
      "Actual class: 4 at 87.59697723388672%\n",
      "Closest attack: 9 at 12.402844429016113%\n",
      "Actual class: 4 at 69.95109558105469%\n",
      "Closest attack: 9 at 30.048715591430664%\n",
      "Actual class: 4 at 20.517335891723633%\n",
      "Closest attack: 9 at 79.48252868652344%\n",
      "\n",
      "Actual class: 8 at 99.99995422363281%\n",
      "Closest attack: 2 at 4.938680285704322e-05%\n",
      "Actual class: 8 at 99.99995422363281%\n",
      "Closest attack: 2 at 4.938680285704322e-05%\n",
      "Actual class: 8 at 99.99934387207031%\n",
      "Closest attack: 2 at 0.0006605011876672506%\n",
      "Actual class: 8 at 99.97357940673828%\n",
      "Closest attack: 2 at 0.02641892433166504%\n",
      "Actual class: 8 at 99.97357940673828%\n",
      "Closest attack: 2 at 0.02641892433166504%\n",
      "Actual class: 8 at 99.8862533569336%\n",
      "Closest attack: 2 at 0.11373730003833771%\n",
      "Actual class: 8 at 99.8436050415039%\n",
      "Closest attack: 2 at 0.15637972950935364%\n",
      "Actual class: 8 at 99.45352935791016%\n",
      "Closest attack: 2 at 0.5464388728141785%\n",
      "Actual class: 8 at 98.28292846679688%\n",
      "Closest attack: 2 at 1.7169870138168335%\n",
      "Actual class: 8 at 97.62841033935547%\n",
      "Closest attack: 2 at 2.371466636657715%\n",
      "Actual class: 8 at 93.33045959472656%\n",
      "Closest attack: 2 at 6.669292449951172%\n",
      "Actual class: 8 at 89.32978820800781%\n",
      "Closest attack: 2 at 10.66989803314209%\n",
      "Actual class: 8 at 83.03700256347656%\n",
      "Closest attack: 2 at 16.96265411376953%\n",
      "Actual class: 8 at 63.893394470214844%\n",
      "Closest attack: 2 at 36.10617446899414%\n",
      "Actual class: 8 at 49.469730377197266%\n",
      "Closest attack: 2 at 50.529884338378906%\n"
     ]
    }
   ],
   "source": [
    "defended_3 = jsmaM(three_tensor, 3, def_model, 20, 1, 0.5)\n",
    "save_image(defended_3[0,0], '../../results/JSMA-M/defended-3.png')\n",
    "print('')\n",
    "defended_4 = jsmaM(four_tensor, 4, def_model, 20, 1, 0.5)\n",
    "save_image(defended_4[0,0], '../../results/JSMA-M/defended-4.png')\n",
    "print('')\n",
    "defended_8 = jsmaM(eight_tensor, 8, def_model, 20, 1, 0.5)\n",
    "save_image(defended_8[0,0], '../../results/JSMA-M/defended-8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted: 2 with 68.77274513244629% certainty. Should be 3\n",
      "The model predicted: 9 with 79.48253154754639% certainty. Should be 4\n",
      "The model predicted: 2 with 50.52988529205322% certainty. Should be 8\n"
     ]
    }
   ],
   "source": [
    "print(f'The model predicted: {def_model(defended_3).argmax().item()} with {def_model(defended_3).max().item() * 100}% certainty. Should be 3')\n",
    "print(f'The model predicted: {def_model(defended_4).argmax().item()} with {def_model(defended_4).max().item() * 100}% certainty. Should be 4')\n",
    "print(f'The model predicted: {def_model(defended_8).argmax().item()} with {def_model(defended_8).max().item() * 100}% certainty. Should be 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figures are obtained:\n",
    "\n",
    "<figure>\n",
    "    <img src=../../results/JSMA-M/defended-3.png width=140>\n",
    "    <figcaption>Classified as a 2</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA-M/defended-4.png width=140>\n",
    "    <figcaption>Classified as a 9</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=../../results/JSMA-M/defended-8.png width=140>\n",
    "    <figcaption>Classified as a 2</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice extremely similar results for the images of 4 and 8.\n",
    "The robust model clearly was not prepared for an attack of this scale.\n",
    "\n",
    "However, we notice the adversarial example that was generated from the 3 looks very different, yet is also classified as a digit other than 3.\n",
    "This implies that the robust model protected against the first attack, yet M-JSMA found another way to attack the model.\n",
    "\n",
    "Overall, we must conclude that this model was not powerful enough to properly stop a M-JSMA, which can be expected, as it was not trained against this attack.\n",
    "Perhaps a model trained on M-JSMA could have defended this attack.\n",
    "\n",
    "Overall, adversarial training did not have much of an effect on M-JSMA.\n",
    "\n",
    "Please see the report for a final discussion and a comparison to other attacks."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d185d99660d43f918a309554ca434be2526a3d95ca2e0b28b0f0a334f8c90ee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
